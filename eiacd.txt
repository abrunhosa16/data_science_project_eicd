from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import numpy as np
import pandas as pd

# Define the models (updated with probability support for ROC/AUC calculation)
models = {
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(probability=True),  # Enable probability estimates for SVM
    'KNN': KNeighborsClassifier(n_neighbors=10)
}

# Define the number of folds and repetitions
n_splits = 5
n_repeats = 10
random_state = 0

# Initialize the RepeatedStratifiedKFold
rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)

# To store the results
results = {model_name: {'accuracy': [], 'roc_auc': [], 'confusion_matrix': [], 'classification_report': []} for model_name in models}

# Evaluate each model using cross-validation
for model_name, model in models.items():
    accuracies = []
    roc_aucs = []
    y_true_all = []
    y_pred_all = []
    y_prob_all = []

    for train_index, test_index in rskf.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else model.decision_function(X_test)
        
        accuracies.append(accuracy_score(y_test, y_pred))
        roc_aucs.append(roc_auc_score(y_test, y_prob))
        y_true_all.extend(y_test)
        y_pred_all.extend(y_pred)
        y_prob_all.extend(y_prob)
    
    # Calculate and store results
    results[model_name]['accuracy'] = accuracies
    results[model_name]['roc_auc'] = roc_aucs
    results[model_name]['confusion_matrix'] = confusion_matrix(y_true_all, y_pred_all)
    results[model_name]['classification_report'] = classification_report(y_true_all, y_pred_all, output_dict=True)















import matplotlib.pyplot as plt
import seaborn as sns

for model_name, result in results.items():
    print(f"\nModel: {model_name}")
    print(f"Mean Accuracy: {np.mean(result['accuracy']):.4f}")
    print(f"Standard Deviation: {np.std(result['accuracy']):.4f}")
    print(f"Min Accuracy: {np.min(result['accuracy']):.4f}")
    print(f"Max Accuracy: {np.max(result['accuracy']):.4f}")
    print(f"Mean ROC AUC: {np.mean(result['roc_auc']):.4f}")
    print(f"Standard Deviation ROC AUC: {np.std(result['roc_auc']):.4f}")
    print(f"Confusion Matrix:\n{result['confusion_matrix']}")
    print(f"Classification Report:\n{pd.DataFrame(result['classification_report']).transpose()}")

# Visualization using Matplotlib and Seaborn
# Boxplot for accuracy and ROC AUC
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.boxplot(data=[results[model]['accuracy'] for model in models.keys()], ax=axes[0])
sns.boxplot(data=[results[model]['roc_auc'] for model in models.keys()], ax=axes[1])
axes[0].set_xticklabels(models.keys(), rotation=45)
axes[0].set_title('Model Accuracy Comparison')
axes[1].set_xticklabels(models.keys(), rotation=45)
axes[1].set_title('Model ROC AUC Comparison')
axes[0].set_ylabel('Accuracy')
axes[1].set_ylabel('ROC AUC')
plt.tight_layout()
plt.show()

# ROC Curves
plt.figure(figsize=(10, 8))
for model_name in models.keys():
    fpr, tpr, _ = roc_curve(y_true_all, y_prob_all)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
